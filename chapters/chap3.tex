%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------     网络中的增量图表征算法     ---------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{网络中的增量图表征算法}
\section{引言}
	社交网络的蓬勃发展使得信息网络研究领域面临越来越多的机遇和挑战。传统的网络特征的表示方法都是以邻接矩阵，拉普拉斯矩阵等离散型或者以中心度，出入度等人工规则来表达，然而在社交网络中随着网络节点的增加，这些表达方法的复杂度和有效性都会出现比较大的问题。近年来机器学习、人工智能的飞速发展，复杂信息网络也因此受益，对网络的表征学习吸引了越来越多的研究兴趣。类似于自然语言中的词向量表征的发展，单词的词频或逆文档频率(TF-IDF)等人工规则表征对于一个单词的描述太过简略，词向量表征则将单词映射成一个向量，向量之间可以计算相似度、并进行加减等操作，对于词的描述在数学意义上更加丰富，图表征学习也是需要从对网络的离散型或人工规则型表达发展出连续性且具有更丰富表征意义的向量表示算法。
	
	本章将提出一种基于高阶接近度的图表征算法，并通过分析不同增量场景的模型变化，提出在增量场景下对应的表征算法。
	
\section{问题描述}
假设一个无向网络$G=(E,V)$,其中$V$表示网络$G$中节点的集合，$E$表示网络$G$中连边的集合，通常图是用邻接矩阵$A \in R^{|V|\times|V|}$表示的，图表征算法的定义就是通过寻找一个映射函数来表征网络中的每个节点：
\begin{equation}
f_G: A \rightarrow X \in R^{|V| \times k} \qquad s.t.\quad k<<|V|
\end{equation}
对于图表征得到的节点表征向量矩阵$X$,需要应用于后续的机器学习任务，不同的机器学习任务对应不同的图挖掘问题，比如对节点向量进行聚类学习，相当于图挖掘领域的社区发现过程；对网络中连边进行分类任务，相当于图挖掘领域的链路预测任务。另外，图表征向量矩阵可用于计算相似度、节点分类、可视化、节点推荐等任务。
\section{静态场景下的图表征算法}
在图表征学习中，最早的方法借鉴与流形学习(manifold learning)的算法，其中Belkin和Niyogi提出的拉普拉斯特征映射是流形学习中的一种典型算法，该算法假设表征映射后得到的表征向量需要保证原网络中一阶接近度，也就是需要保证网络中互为邻居的节点对连边权重越大，那么在表征向量空间就应该保证越接近，也即优化如下问题：
\begin{equation}\label{laplacian_st}
\begin{aligned}
\min_{X^TDX=I} \phi(X) &= \frac{1}{2}\sum_{i,j}|X_i - X_j|^2A_{ij} \\
&= \frac{1}{2}(\sum_{i,j}(x_i^2+x_j^2-2x_ix_j) A_{ij}) \\
&=\frac{1}{2} (\sum_ix_i^2D_{ii} +\sum_j x_j^2 D_{jj} - 2\sum_{i,j}x_i x_j A_{ij}) \\
&= tr(X^TLX)
\end{aligned}
\end{equation}
其中$X$为表征向量矩阵，$A$为图的邻接矩阵，$D$为对角元素为节点度的对角阵，$L$为拉普拉斯矩阵。对式(\ref{laplacian_st}),根据谱图理论\cite{chung1997spectral}可以通过拉普拉斯算子的最值特征向量来进行逼近，上述问题可以转化为(推导过程如式(\ref{laplacian_reduce}))：
\begin{equation}
\max_{U^TU = I} tr(U^TWU)
\end{equation}
其中$W = D^{-\frac{1}{2}}AD^{-\frac{1}{2}} $。为得到图表征向量就转变成求$W$矩阵的前$k$大的特征值所对应的特征向量。

拉普拉斯特征映射假设中只考虑到保留一阶接近度，对于图表征来说，一阶接近度所表征的信息是远远不够的，因此需要引进更高阶的接近度来表征向量，比如LINE算法引入了二阶接近度；DeepWalk算法引进了$l$(游走半径)阶接近度，对比实验也验证了在保留高阶接近度的算法中后续的节点分类等机器学习任务效果更好。

\subsection{基于高阶接近度的拉普拉斯特征映射}
在这一部分，主要集中在无向图中的图表征过程，因此在保证得到节点的表征向量的同时，对称性也在考虑之列。基于对称性这个基本假设，提出基于拉普拉斯特征映射改进的目标函数，用来保留高阶接近度：
\begin{equation}\label{high_order_condition}
%\begin{aligned}
	\min_{X^TD'X=I} \phi(X) = \frac{1}{2}\sum_{i,j}|X_i - X_j|^2S_{ij} 
%	&= \frac{1}{2}(\sum_{i,j}(y_i^2+y_j^2-2y_iy_j) S_{ij}) \\
%	&=\frac{1}{2} (\sum_iy_i^2D'_{ii} +\sum_j y_j^2 D'_{jj} - 2\sum_{i,j}y_i y_j S_{ij}) \\
%	&= tr(X^T(D'-S)X)
%\end{aligned}
\end{equation}
其中约束条件中的$D'$为区别于$S$矩阵为邻接矩阵时对应的对角矩阵$D$，根据不同的$S$矩阵进调整。$S$矩阵是通过保留高阶接近度的计算方法得到的，比如共同邻居(Common Neighbors,CN)\cite{newman2001clustering}、Adamic-Adar系数(AA)\cite{adamic2003friends}、资源分配系数(Resource Allocation,RA)\cite{zhou2009predicting}、Jaccard系数、Katz系数、个性化Pagerank(Personalized Pagerank)\cite{wang2015link}等方式，后文将称$S$矩阵为\textbf{相似度矩阵$S$}。下面将重点介绍前三种计算方法。

\definition{\textbf{共同邻居}:}
共同邻居计算方法是目前用来衡量节点接近度中使用最广泛的之一，因为这种计算非常简单，节点对$i$,$j$之间的共同邻居值就是跟$i$,$j$都有直接连边的节点数，也即：
\begin{equation}
	CN(i,j) = |N(i) \cap N(j)|
\end{equation}
其中$N(i)$表示节点$i$的邻居节点集合，$|\cdot|$表示对集合计数。将共同邻居这个计算方法推广至矩阵形式，可以得到用来保留高阶接近度的相似度矩阵$S_{CN}$:
\begin{equation}
	S_{CN}(A) = A^2
\end{equation}
其中$A$为无向网络的邻接矩阵。
\definition{\textbf{Adamic-Adar系数}:}Adamic-Adar系数最先是由Adamic和Adar提出来用来计算两个网页之间的相似度的，后来被广泛应用与社交网络分析之中。Adamic-Adar系数的计算方法类似于共同邻居，不同的是对不同的共同邻居采用不同的权重，该系数假定度较高的节点在共同邻居中所占权重较低，权重值为对数节点度的倒数，也即：
\begin{equation}
	AA(i,j) = \sum_{u \in N(i)\cap N(j)} \frac{1}{\log(|N(u)|)}
\end{equation}
其中$|N(u)|$也称为节点$u$的度。同样地，将Adamic-Adar系数计算方法推广至矩阵形式，可以得到对应用来保留高阶接近度的相似度矩阵$S_{AA}$:
\begin{equation}
	S_{AA}(A) = A \cdot M \cdot A
\end{equation}
其中$M$为对角矩阵，且有：
\begin{equation}
M_{ii} = 1/\log(\sum_j{A_{ij}})
\end{equation}

\definition{\textbf{资源分配系数}:}资源分配系数最早是基于物理过程中的资源分配得来的一个计算法方法，过程类似于Adamic-Adar系数的计算方法，同样是给度较高的节点惩罚权重，资源分配系数采取的权重是节点度的倒数，因此可以看出来资源分配系数对度高的节点惩罚相较之下比较重，度处在平均水平的节点两中计算方法的惩罚权重相当。资源分配系数的计算方法如下：
\begin{equation}
	RA(i,j) =  \sum_{u \in N(i)\cap N(j)} \frac{1}{|N(u)|}
\end{equation}
同样地，可以将资源分配系数推广至矩阵形式，得到可以用来保留高阶接近度的相似度矩阵$S_{RA}$:
\begin{equation}
	S_{RA}(A) = A\cdot K \cdot A
\end{equation}
其中$K$为对角矩阵，且有：
\begin{equation}
	K_{ii} = 1/\sum_jA_{ij}
\end{equation}

根据以上三种系数的介绍和定义\ref{first_order}和定义\ref{second_order}中介绍的接近度概念，在共同邻居的计算方法中引入了二阶接近度，也即二度邻居的信息，在Adamic-Adar系数和资源分配系数中，不仅计算了共同邻居，还对共同邻居的度(邻居总数)进行了计算和处理，也即处理了高于二阶接近度的高阶接近度信息。

考虑式\ref{high_order_condition}，当采用不同的相似度矩阵$S$时，$D'$满足：
\begin{equation}
D'_{ii} = \sum_j{S_{ij}}
\end{equation}
于是同式(\ref{laplacian_reduce})推导，则原问题变成：
\begin{equation}
\max_{U^TU = I} tr(U^TWU)
\end{equation}
其中$W = D^{\prime-\frac{1}{2}}SD^{\prime-\frac{1}{2}}$,表征向量矩阵为$W$前$k$个特征值对应的特征向量组合而成的矩阵。

%\algorithm{基于高阶接近度的拉普拉斯特征映射HLE}
\begin{figure}[htb]
	\centering
	\begin{minipage}{.7\linewidth}
		\begin{algorithm}[H]
			\small
			\caption{HLE算法}
			\begin{algorithmic}[1]
				\Require
				\Statex $\mathcal{A}$ :邻接矩阵
				\Statex $k$ : 表征向量维数
				\Ensure
				\Statex $\mathcal{X}$ :表征向量矩阵
				\Statex
				\State $\mathcal{S} \leftarrow S_{CN}(A) /S_{AA}(A) / S_{RA}(A)$
				\State $D'_{ii} = \sum_j S_{ij}$
				\State $W \leftarrow  D^{\prime-\frac{1}{2}}SD^{\prime-\frac{1}{2}}$
				\State 通过特征值分解得到$W$前k维特征向量，组合成矩阵 $\mathcal{X}$
				\State 返回 $\mathcal{X}$
			\end{algorithmic}
		\end{algorithm}
	\end{minipage}
\end{figure}

\section{增量场景下的图表征算法}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------     本章小结     ---------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{本章小结}
本章从统计学角度，通过对实际数据的分析，深入探讨了第三方库与移动应用的关系。评分作为移动应用最重要的评价方式之一，是开发者最为关注和在意的方面。因此，任何能够提高移动应用评分的方式都具有现实意义和价值。第三方库作为本文的研究对象，其与移动应用评分的关系是本文研究的基础和出发点。本章的分析，其目的是为了对本文研究问题的依据进行验证。为了比较不同评分的移动应用间使用第三方库的差异，本章将移动应用根据评分高低分成了四个类别，然后对这四个类别的移动应用，分别从第三方库的个体使用量和平均使用量两个方面进行检验分析。检验结果显示了高评分类别的移动应用与低评分类别的显著差异。本章亦对高评分类别的移动应用进行更为细粒度的分析，从一星作为步长减少到半星作为步长，将高评分类别的移动应用分成更小的四个集合。在此基础上，通过两两对比检验，结果表现出与预想一致的结果，即在高评分类别中，就第三方库使用情况，其内部同样具有显著的差异。经过大小对比组的检验分析，本章得出以下结论：\textbf{使用第三方库的移动应用确实具有更高的评分。}此结论回答了本章开始的研究问题，且验证了本文研究问题的依据，为本文的研究工作奠定了基础。本章的检验分析结果可以为其他研究者所用，为其之后在第三方库相关问题的研究中提供数据支撑。