%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------     网络中的增量图表征算法     ---------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{网络中的增量图表征算法}
\section{引言}
	社交网络的蓬勃发展使得信息网络研究领域面临越来越多的机遇和挑战。传统的网络特征的表示方法都是以邻接矩阵，拉普拉斯矩阵等离散型或者以中心度，出入度等人工规则来表达，然而在社交网络中随着网络节点的增加，这些表达方法的复杂度和有效性都会出现比较大的问题。近年来机器学习、人工智能的飞速发展，复杂信息网络也因此受益，对网络的表征学习吸引了越来越多的研究兴趣。类似于自然语言中的词向量表征的发展，单词的词频或逆文档频率(TF-IDF)等人工规则表征对于一个单词的描述太过简略，词向量表征则将单词映射成一个向量，向量之间可以计算相似度、并进行加减等操作，对于词的描述在数学意义上更加丰富，图表征学习也是需要从对网络的离散型或人工规则型表达发展出连续性且具有更丰富表征意义的向量表示算法。对于图表征得到的节点表征向量矩阵,需要应用于后续的机器学习任务，不同的机器学习任务对应不同的图挖掘问题，比如对节点向量进行聚类学习，相当于图挖掘领域的社区发现过程；对网络中连边进行分类任务，相当于图挖掘领域的链路预测任务。另外，图表征向量矩阵可用于计算相似度、节点分类、可视化、节点推荐等任务。
	
	本章将提出一种基于高阶接近度的图表征算法，并通过分析不同增量场景的模型变化，提出在增量场景下对应的表征算法。
	
\section{问题描述}
假设一个无向网络$G=(E,V)$,其中$V$表示网络$G$中节点的集合，$E$表示网络$G$中连边的集合，通常图是用邻接矩阵$A \in R^{|V|\times|V|}$表示的，图表征算法的定义就是通过寻找一个映射函数来表征网络中的每个节点：
\begin{equation}
f_G: \textbf{A} \rightarrow \textbf{X} \in R^{|V| \times k} \qquad s.t.\quad k<<|V|
\end{equation}
对于真实应用中的网络往往是存在动态变化的,对于传统的图表征算法来说，真实场景下的动态网络会带来很大的挑战，传统算法基本都是离线模型，当新数据进入时，需要重新对全部数据进行批量处理。在网络规模很大时，离线模型是不具备伸缩性的，也不适合于在线应用中使用。因此对于大型动态网络需要提出一种增量式的学习框架来实现快速、可扩展的表征学习。

为了方便描述统一字符使用规范及意义，用大写字母加粗表示矩阵，如$\textbf{A}$；用小写字母加粗表示向量，如$\textbf{a}$；用普通小写字母代表标量，如$a$；矩阵$\textbf{A}$的第$i$行用$\text{A}(i,:)$表示；矩阵$\textbf{A}$的第$j$列用$\textbf{A}(:,j)$表示，也可以简写为$\textbf{A}_j$；矩阵$\textbf{A}$中第$i$行第$j$列元素用$\textbf{A}(i,j)$表示，也可以简写为$\textbf{A}_{ij}$，矩阵$\textbf{A}$的转置记为$\textbf{A}^T$，矩阵$\textbf{A}$的迹记为$tr(\textbf{A})$，$\textbf{I}$代表单位矩阵；常用的字符表如下表：
\begin{table}
	\centering
	\caption{常用字符及其代表含义}
	\begin{tabular}{|C{1.8in}|C{3.3in}|}
		\hline
		\textbf{字符} & \textbf{含义} \\ \hline\hline
		$G^{(t)}$ & 时刻$t$时的网络数据 \\ \hline
		$G^{(t+1)}$& 时刻$t+1$时的网络数据  \\ \hline
		$\textbf{A}^{(t)}$ & 时刻$t$时网络的邻接矩阵 \\ \hline
		$\textbf{A}^{(t+1)}$ & 时刻$t+1$时网络的邻接矩阵 \\ \hline
		$\Delta\textbf{A}$ & 时刻$t$到时刻$t+1$之间邻接矩阵的变化 \\ \hline
		$\textbf{X}^{(t)}$ & 时刻$t$时网络的表征向量矩阵 \\ \hline
		% $\textbf{X}^{t+1}$ & 时刻$t+1$时网络的表征向量矩阵 \\ \hline
		$k$ & 节点向量表征的维数  \\ \hline
	\end{tabular}
\end{table}

用$V^{(t)} = \{v_1, v_2,\cdots,v_n\}$表示在时刻$t$网络$G^{(t)}$中的节点集合，用$\textbf{A}^{(t)}$来表示时刻$t$的网络结构，随着时刻$t$的变化,网络的拓扑结构会发生变化，当网络中节点数目不变时，用$\Delta\textbf{A}$表示邻接矩阵从时刻$t$到时刻$t+1$的变化；当网络中有新增节点的时候，需要对邻接矩阵$\textbf{A}^{(t)}$进行增广再计算变化量。如前文中提到的，在动态网络中重复用离线模型是不适合在线应用的，那么对于动态网络的学习过程应该分成两个部分：1.静态网网络的学习过程；2.增量网络的学习过程；也即转化成如下两个问题：
\begin{itemize}
	\item \textbf{问题1}：静态场景下的图表征算法。给定网络的邻接矩阵$\textbf{A}^{(t)}$,对于所有节点学习网络的表征向量矩阵$\textbf{X}^{(t)}$。
	\item \textbf{问题2}: 增量场景下的图表征算法。给定网络某时刻$t+1$的邻接矩阵$\textbf{A}^{(t+1)}$，
		和上一时刻的表征向量$\textbf{X}^{(t)}$，对于所有节点学习网络的表征向量矩阵$\textbf{X}^{(t+1)}$
\end{itemize}

下面章节将就这两个问题展开讨论分析。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------     静态场景下的图表征算法     ---------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{静态场景下的图表征算法}
在图表征学习中，最早的方法借鉴与流形学习(manifold learning)的算法，其中Belkin和Niyogi提出的拉普拉斯特征映射是流形学习中的一种典型算法，该算法假设表征映射后得到的表征向量需要保证原网络中一阶接近度，也就是需要保证网络中互为邻居的节点对连边权重越大，那么在表征向量空间就应该保证越接近，也即优化如下问题：
\begin{equation}\label{laplacian_st}
\begin{aligned}
\min_{X^TDX=I} \phi(X) &= \frac{1}{2}\sum_{i,j}|X_i - X_j|^2A_{ij} \\
&= \frac{1}{2}(\sum_{i,j}(x_i^2+x_j^2-2x_ix_j) A_{ij}) \\
&=\frac{1}{2} (\sum_ix_i^2D_{ii} +\sum_j x_j^2 D_{jj} - 2\sum_{i,j}x_i x_j A_{ij}) \\
&= tr(X^TLX)
\end{aligned}
\end{equation}
其中$X$为表征向量矩阵，$A$为图的邻接矩阵，$D$为对角元素为节点度的对角阵，$L$为拉普拉斯矩阵。对式(\ref{laplacian_st}),根据谱图理论\cite{chung1997spectral}可以通过拉普拉斯算子的最值特征向量来进行逼近，上述问题可以转化为(推导过程如式(\ref{laplacian_reduce}))：
\begin{equation}
\max_{X^TX = I} tr(X^TWX)
\end{equation}
其中$W = D^{-\frac{1}{2}}AD^{-\frac{1}{2}} $。为得到图表征向量就转变成求$W$矩阵的前$k$大的特征值所对应的特征向量。

拉普拉斯特征映射假设中只考虑到保留一阶接近度，对于图表征来说，一阶接近度所表征的信息是远远不够的，因此需要引进更高阶的接近度来表征向量，比如LINE算法引入了二阶接近度；DeepWalk算法引进了$l$(游走半径)阶接近度，对比实验也验证了在保留高阶接近度的算法中后续的节点分类等机器学习任务效果更好。

\subsection{基于高阶接近度的拉普拉斯特征映射}
在这一部分，主要集中在无向图中的图表征过程，因此在保证得到节点的表征向量的同时，对称性也在考虑之列。基于对称性这个基本假设，提出基于拉普拉斯特征映射改进的目标函数，用来保留高阶接近度：
\begin{equation}\label{high_order_condition}
%\begin{aligned}
	\min_{X^TD'X=I} \phi(X) = \frac{1}{2}\sum_{i,j}|X_i - X_j|^2S_{ij} 
%	&= \frac{1}{2}(\sum_{i,j}(y_i^2+y_j^2-2y_iy_j) S_{ij}) \\
%	&=\frac{1}{2} (\sum_iy_i^2D'_{ii} +\sum_j y_j^2 D'_{jj} - 2\sum_{i,j}y_i y_j S_{ij}) \\
%	&= tr(X^T(D'-S)X)
%\end{aligned}
\end{equation}
其中约束条件中的$D'$为区别于$S$矩阵为邻接矩阵时对应的对角矩阵$D$，根据不同的$S$矩阵进调整。$S$矩阵是通过保留高阶接近度的计算方法得到的，比如共同邻居(Common Neighbors,CN)\cite{newman2001clustering}、Adamic-Adar系数(AA)\cite{adamic2003friends}、资源分配系数(Resource Allocation,RA)\cite{zhou2009predicting}、Jaccard系数、Katz系数、个性化Pagerank(Personalized Pagerank)\cite{wang2015link}等方式，后文将称$S$矩阵为\textbf{相似度矩阵$S$}。下面将重点介绍前三种计算方法。

\definition{\textbf{共同邻居}:}
共同邻居计算方法是目前用来衡量节点接近度中使用最广泛的之一，因为这种计算非常简单，节点对$i$,$j$之间的共同邻居值就是跟$i$,$j$都有直接连边的节点数，也即：
\begin{equation}
	CN(i,j) = |N(i) \cap N(j)|
\end{equation}
其中$N(i)$表示节点$i$的邻居节点集合，$|\cdot|$表示对集合计数。将共同邻居这个计算方法推广至矩阵形式，可以得到用来保留高阶接近度的相似度矩阵$S_{CN}$:
\begin{equation}
	S_{CN}(A) = A^2
\end{equation}
其中$A$为无向网络的邻接矩阵。
\definition{\textbf{Adamic-Adar系数}:}Adamic-Adar系数最先是由Adamic和Adar提出来用来计算两个网页之间的相似度的，后来被广泛应用与社交网络分析之中。Adamic-Adar系数的计算方法类似于共同邻居，不同的是对不同的共同邻居采用不同的权重，该系数假定度较高的节点在共同邻居中所占权重较低，权重值为对数节点度的倒数，也即：
\begin{equation}
	AA(i,j) = \sum_{u \in N(i)\cap N(j)} \frac{1}{\log(|N(u)|)}
\end{equation}
其中$|N(u)|$也称为节点$u$的度。同样地，将Adamic-Adar系数计算方法推广至矩阵形式，可以得到对应用来保留高阶接近度的相似度矩阵$S_{AA}$:
\begin{equation}
	S_{AA}(A) = A \cdot M \cdot A
\end{equation}
其中$M$为对角矩阵，且有：
\begin{equation}
M_{ii} = 1/\log(\sum_j{A_{ij}})
\end{equation}

\definition{\textbf{资源分配系数}:}资源分配系数最早是基于物理过程中的资源分配得来的一个计算法方法，过程类似于Adamic-Adar系数的计算方法，同样是给度较高的节点惩罚权重，资源分配系数采取的权重是节点度的倒数，因此可以看出来资源分配系数对度高的节点惩罚相较之下比较重，度处在平均水平的节点两中计算方法的惩罚权重相当。资源分配系数的计算方法如下：
\begin{equation}
	RA(i,j) =  \sum_{u \in N(i)\cap N(j)} \frac{1}{|N(u)|}
\end{equation}
同样地，可以将资源分配系数推广至矩阵形式，得到可以用来保留高阶接近度的相似度矩阵$S_{RA}$:
\begin{equation}
	S_{RA}(A) = A\cdot K \cdot A
\end{equation}
其中$K$为对角矩阵，且有：
\begin{equation}
	K_{ii} = 1/\sum_jA_{ij}
\end{equation} 

根据以上三种系数的介绍和定义\ref{first_order}和定义\ref{second_order}中介绍的接近度概念，在共同邻居的计算方法中引入了二阶接近度，也即二度邻居的信息，在Adamic-Adar系数和资源分配系数中，不仅计算了共同邻居，还对共同邻居的度(邻居总数)进行了计算和处理，也即处理了高于二阶接近度的高阶接近度信息。为方便后文进行推导，可以将三种相似度矩阵$\textbf{S}$计算方法表达成统一的形式：
\begin{equation}\label{unify}
	\textbf{S} = \textbf{A} \cdot \textbf{M} \cdot \textbf{A}
\end{equation}
对应的三种相似度矩阵$S$的表格如下：
\begin{table}
	\centering
	\caption{三种相似度矩阵计算方法}
	\begin{tabular}{|C{1.0in}|C{1.8in}|C{1.8in}|}
		\hline
		$\textbf{S}$ & 表达式 & $\textbf{M}$的值  \\ \hline\hline
		$\textbf{S}_{CN}$ & $\textbf{A}\cdot \textbf{M}_{CN} \cdot\textbf{A}$ & $\textbf{I}$  \\ \hline
		$\textbf{S}_{AA}$ & $\textbf{A}\cdot \textbf{M}_{AA}\cdot \textbf{A}$ & $\textbf{M}_{ii} = 1/\log(\sum_j{\textbf{A}_{ij}})$ \\ \hline
		$\textbf{S}_{RA}$ & $\textbf{A}\cdot \textbf{M}_{RA}\cdot \textbf{A}$ & $\textbf{M}_{ii} = 1/\sum_j\textbf{A}_{ij}$ \\ \hline
	\end{tabular}
\end{table}

考虑式\ref{high_order_condition}，当采用不同的相似度矩阵$S$时，$D'$满足：
\begin{equation}
D'_{ii} = \sum_j{S_{ij}}
\end{equation}
于是同式(\ref{laplacian_reduce})推导，则原问题变成：
\begin{equation}\label{decomp}
\max_{X^TX = I} tr(X^TWX)
\end{equation}
其中$W = D^{\prime-\frac{1}{2}}SD^{\prime-\frac{1}{2}}$,上述问题转化为求矩阵$W$的特征值问题，也即对矩阵$W$进行特征值分解：
\begin{equation}\label{evol}
	W = X^T \Lambda X 
\end{equation}
其中$\Lambda$为对角矩阵，对角线元素为矩阵$W$的特征值，$X$为正交矩阵，也即表征向量矩阵。对于式(\ref{decomp})的最大值问题相当于求解$W$最接近特征值分解形式，也即矩阵$X$为$W$前$k$个特征值对应的特征向量组合而成的矩阵。

\subsection{HLE算法描述}
	给定图数据$G = (E,v)$和图表征向量维数$k$，其中图$G$以邻接矩阵$A\in R^{|V|\times |V|}$表示，HLE算法将得到图$G$的向量表征$X\in |V|\times k$。
	
	在算法开始时，根据相似度准则的选择，令相似度矩阵$S$分别为共同邻居$S_{Cn}$、Adamic-Adar系数$S_{AA}$或资源分配系数$S_{RA}$中的一个。根据$S$矩阵计算出对应的对角矩阵$D\prime$,使得$D'_{ii} = \sum_j S_{ij}$。

	进一步计算矩阵$W$，使得$W=D^{\prime-\frac{1}{2}}SD^{\prime-\frac{1}{2}}$，对W矩阵进行特征值分解得到前k个最大特征值对应的特征列向量$\{X_1,X_2,\cdots,X_k\}$,组合成图表征向量矩阵$X$。
	
	HLE算法中根据不同的接近度要求可以选择不同的相似度计算方法，其中共同邻居保留二阶接近度，Adamic-Adar系数和资源分配系数保留网络的三阶接近度。


%\algorithm{基于高阶接近度的拉普拉斯特征映射HLE}
\begin{figure}[htb]
	\centering
	\begin{minipage}{.7\linewidth}
		\begin{algorithm}[H]
			\small
			\caption{HLE算法}
			\begin{algorithmic}[1]
				\Require
				\Statex $\mathcal{A}$ :邻接矩阵
				\Statex $k$ : 表征向量维数
				\Ensure
				\Statex $\mathcal{X}$ :表征向量矩阵
				\Statex
				\State $\mathcal{S} \leftarrow S_{CN}(A) /S_{AA}(A) / S_{RA}(A)$
				\State $D'_{ii} = \sum_j S_{ij}$
				\State $W \leftarrow  D^{\prime-\frac{1}{2}}SD^{\prime-\frac{1}{2}}$
				\State 通过特征值分解得到$W$前k个特征向量，组合成矩阵 $\mathcal{X}$
				\State 返回 $\mathcal{X}$
			\end{algorithmic}
		\end{algorithm}
	\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------     增量场景下的图表征算法     ---------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{增量场景下的图表征算法}
前面一节提出了应用高阶接近度的拉普拉斯特征映射方法来对网络进行表征，对于真实应用中的网络往往是存在动态变化的。比如在社交媒体上，网络中时刻会存在用户进行更新状态、添加好友等操作，这些操作都是会引起社交关系发生变化的，从数学角度看就是网络中的连边会出现增删等情况；除此之外社交媒体中，也会存在有新的用户注册加入的情况，这种情况即对应网络中有新加入的节点。对于传统的图表征算法来说，真实场景下的动态网络会带来很大的挑战，传统算法基本都是离线模型，对于变化后的网络只能进行重新计算或学习，当网络规模很大时，离线模型是不具备伸缩性的，也不适合于在线应用中使用。因此对于大型动态网络需要提出一种增量式的学习方式来实现快速、可扩展的表征学习。

下面将提出一种增量的图表征算法，应用于增量场景下的图表征学习，在这里本文采用文献\cite{chi2007evolutionary}中采用的假设，认为在时刻$t$和时刻$t+1$之间网络的变化是平缓而微小的。如前文中提到的，网络中的变化场景分两类：1. 节点数不变，网络连边的增删；2.节点数的新增；因此下面将分别就这两种情况进行分析和处理。

\subsection{节点数不变}
本文的研究对象是无向网络，根据式(\ref{evol}),在时刻$t$网络表征学习相当对矩阵$\textbf{W}^{(t)}$进行特征值分解，也即：
\begin{equation}
	\textbf{W}^{(t)} = \textbf{X}^{(t)T} \Lambda^{(t)} \textbf{X} ^{(t)}
\end{equation}
其中$\textbf{X} ^{(t)}$为时刻$t$是网络的表征向量，其中$\textbf{W}^{(t)} = \textbf{D}^{\prime(t)-\frac{1}{2}}\textbf{S}^{(t)}\textbf{D}^{\prime(t)-\frac{1}{2}}$；在网络节点不变的情况下，用$\Delta\textbf{W}$表示随机





根据式(\ref{unify})对不同个相似度矩阵的统一表示,



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------     本章小结     ---------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{本章小结}
本章从统计学角度，通过对实际数据的分析，深入探讨了第三方库与移动应用的关系。评分作为移动应用最重要的评价方式之一，是开发者最为关注和在意的方面。因此，任何能够提高移动应用评分的方式都具有现实意义和价值。第三方库作为本文的研究对象，其与移动应用评分的关系是本文研究的基础和出发点。本章的分析，其目的是为了对本文研究问题的依据进行验证。为了比较不同评分的移动应用间使用第三方库的差异，本章将移动应用根据评分高低分成了四个类别，然后对这四个类别的移动应用，分别从第三方库的个体使用量和平均使用量两个方面进行检验分析。检验结果显示了高评分类别的移动应用与低评分类别的显著差异。本章亦对高评分类别的移动应用进行更为细粒度的分析，从一星作为步长减少到半星作为步长，将高评分类别的移动应用分成更小的四个集合。在此基础上，通过两两对比检验，结果表现出与预想一致的结果，即在高评分类别中，就第三方库使用情况，其内部同样具有显著的差异。经过大小对比组的检验分析，本章得出以下结论：\textbf{使用第三方库的移动应用确实具有更高的评分。}此结论回答了本章开始的研究问题，且验证了本文研究问题的依据，为本文的研究工作奠定了基础。本章的检验分析结果可以为其他研究者所用，为其之后在第三方库相关问题的研究中提供数据支撑。

